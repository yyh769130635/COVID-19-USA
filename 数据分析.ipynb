{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as func\n",
    " \n",
    "def toDate(inputStr):\n",
    "    newStr = \"\"\n",
    "    if len(inputStr) == 8:\n",
    "        s1 = inputStr[0:4]\n",
    "        s2 = inputStr[5:6]\n",
    "        s3 = inputStr[7]\n",
    "        newStr = s1+\"-\"+\"0\"+s2+\"-\"+\"0\"+s3\n",
    "    else:\n",
    "        s1 = inputStr[0:4]\n",
    "        s2 = inputStr[5:6]\n",
    "        s3 = inputStr[7:]\n",
    "        newStr = s1+\"-\"+\"0\"+s2+\"-\"+s3\n",
    "    date = datetime.strptime(newStr, \"%Y-%m-%d\")\n",
    "    return date\n",
    " \n",
    " \n",
    " \n",
    "#主程序:\n",
    "spark = SparkSession.builder.config(conf = SparkConf()).getOrCreate()\n",
    " \n",
    "fields = [StructField(\"date\", DateType(),False),StructField(\"county\", StringType(),False),StructField(\"state\", StringType(),False),\n",
    "                    StructField(\"cases\", IntegerType(),False),StructField(\"deaths\", IntegerType(),False),]\n",
    "schema = StructType(fields)\n",
    " \n",
    "rdd0 = spark.sparkContext.textFile(\"/user/hadoop/us-counties.txt\")\n",
    "rdd1 = rdd0.map(lambda x:x.split(\"\\t\")).map(lambda p: Row(toDate(p[0]),p[1],p[2],int(p[3]),int(p[4])))\n",
    " \n",
    " \n",
    "shemaUsInfo = spark.createDataFrame(rdd1,schema)\n",
    " \n",
    "shemaUsInfo.createOrReplaceTempView(\"usInfo\")\n",
    " \n",
    "#1.计算每日的累计确诊病例数和死亡数\n",
    "df = shemaUsInfo.groupBy(\"date\").agg(func.sum(\"cases\"),func.sum(\"deaths\")).sort(shemaUsInfo[\"date\"].asc())\n",
    " \n",
    "#列重命名\n",
    "df1 = df.withColumnRenamed(\"sum(cases)\",\"cases\").withColumnRenamed(\"sum(deaths)\",\"deaths\")\n",
    "df1.repartition(1).write.json(\"result1.json\")                               #写入hdfs\n",
    " \n",
    "#注册为临时表供下一步使用\n",
    "df1.createOrReplaceTempView(\"ustotal\")\n",
    " \n",
    "#2.计算每日较昨日的新增确诊病例数和死亡病例数\n",
    "df2 = spark.sql(\"select t1.date,t1.cases-t2.cases as caseIncrease,t1.deaths-t2.deaths as deathIncrease from ustotal t1,ustotal t2 where t1.date = date_add(t2.date,1)\")\n",
    " \n",
    "df2.sort(df2[\"date\"].asc()).repartition(1).write.json(\"result2.json\")           #写入hdfs\n",
    " \n",
    "#3.统计截止5.19日 美国各州的累计确诊人数和死亡人数\n",
    "df3 = spark.sql(\"select date,state,sum(cases) as totalCases,sum(deaths) as totalDeaths,round(sum(deaths)/sum(cases),4) as deathRate from usInfo  where date = to_date('2020-05-19','yyyy-MM-dd') group by date,state\")\n",
    " \n",
    "df3.sort(df3[\"totalCases\"].desc()).repartition(1).write.json(\"result3.json\") #写入hdfs\n",
    " \n",
    "df3.createOrReplaceTempView(\"eachStateInfo\")\n",
    " \n",
    "#4.找出美国确诊最多的10个州\n",
    "df4 = spark.sql(\"select date,state,totalCases from eachStateInfo  order by totalCases desc limit 10\")\n",
    "df4.repartition(1).write.json(\"result4.json\")\n",
    " \n",
    "#5.找出美国死亡最多的10个州\n",
    "df5 = spark.sql(\"select date,state,totalDeaths from eachStateInfo  order by totalDeaths desc limit 10\")\n",
    "df5.repartition(1).write.json(\"result5.json\")\n",
    " \n",
    "#6.找出美国确诊最少的10个州\n",
    "df6 = spark.sql(\"select date,state,totalCases from eachStateInfo  order by totalCases asc limit 10\")\n",
    "df6.repartition(1).write.json(\"result6.json\")\n",
    " \n",
    "#7.找出美国死亡最少的10个州\n",
    "df7 = spark.sql(\"select date,state,totalDeaths from eachStateInfo  order by totalDeaths asc limit 10\")\n",
    "df7.repartition(1).write.json(\"result7.json\")\n",
    " \n",
    "#8.统计截止5.19全美和各州的病死率\n",
    "df8 = spark.sql(\"select 1 as sign,date,'USA' as state,round(sum(totalDeaths)/sum(totalCases),4) as deathRate from eachStateInfo group by date union select 2 as sign,date,state,deathRate from eachStateInfo\").cache()\n",
    "df8.sort(df8[\"sign\"].asc(),df8[\"deathRate\"].desc()).repartition(1).write.json(\"result8.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
